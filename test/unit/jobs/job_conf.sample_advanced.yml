# TODO: translocate docs for all pulsar embedded
# TODO: translocate docs for all kubernetes
# TODO: implement and demo embedded job_metrics.
# TODO: translocate docker option docs into docker_local environment description
# TODO: translocate singluarity option docs into singularity_local environment description
# TODO: all destinations from condor on
runners:
  local:
    load: galaxy.jobs.runners.local:LocalJobRunner
    workers: 4
  drmaa:
    load: galaxy.jobs.runners.drmaa:DRMAAJobRunner

    # Different DRMs handle successfully completed jobs differently,
    # these options can be changed to handle such differences and
    # are explained in detail on the Galaxy wiki. Defaults are shown
    invalidjobexception_state: ok
    invalidjobexception_retries: 0
    internalexception_state: ok
    internalexception_retries: 0
  sge:
    load: galaxy.jobs.runners.drmaa:DRMAAJobRunner
    # Override the $DRMAA_LIBRARY_PATH environment variable
    drmaa_library_path: /sge/lib/libdrmaa.so
  cli: 
    load: galaxy.jobs.runners.cli:ShellJobRunner
  condor: 
    load: galaxy.jobs.runners.condor:CondorJobRunner
  slurm: 
    load: galaxy.jobs.runners.slurm:SlurmJobRunner
  dynamic:
    # The dynamic runner is not a real job running plugin and is
    # always loaded, so it does not need to be explicitly stated in
    # runners. However, if you wish to change the base module
    # containing your dynamic rules, you can do so.
    # The `load` attribute is not required (and ignored if
    # included).
    rules_module: galaxy.jobs.rules

  godocker:
    # Go-Docker is a batch computing/cluster management tool using Docker
    # See https://bitbucket.org/osallou/go-docker for more details.
    load: galaxy.jobs.runners.godocker:GodockerJobRunner
    # Specify the instance of GoDocker
    godocker_master: GODOCKER_URL
    # GoDocker username
    user: USERNAME
    # GoDocker API key
    key: APIKEY
    # Specify the project present in the GoDocker setup
    godocker_project: galaxy

  chronos:
    # Chronos is a framework for the Apache Mesos software; a software which manages
    # computer clusters. Specifically, Chronos runs of top of Mesos and it's used
    # for job orchestration.
    # 
    # This runner requires a shared file system where the directories of
    # `job_working_directory`, `file_path` and `new_file_path` settings defined on
    # the `galaxy.ini` file are shared amongst the Mesos agents (i.e. nodes which
    # actually run the jobs).
    load: galaxy.jobs.runners.chronos:ChronosJobRunner
    # Hostname which runs Chronos instance.
    chronos: '`chronos_host`'
    # The email address of the person responsible for the job.
    owner: foo@bar.com
    # Username to access Mesos cluster.
    username: username
    # Password to access Mesos cluster.
    password: password
    # True to communicate with Chronos over HTTPS; false otherwise
    insecure: true

  # Pulsar runners (see more at https://pulsar.readthedocs.io/)
  pulsar_rest:
    load: galaxy.jobs.runners.pulsar:PulsarRESTJobRunner
    # Allow optimized HTTP calls with libcurl (defaults to urllib)
    transport: curl
    # Experimental Caching*: Undocumented, don't use.
    #cache: false

  pulsar_mq:
    load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
    # AMQP URL to connect to.
    amqp_url: amqp://guest:guest@localhost:5672//
    # URL remote Pulsar apps should transfer files to this Galaxy
    # instance to/from. This can be unspecified/empty if
    # `galaxy_infrastructure_url1 is set in galaxy.yml.
    galaxy_url: http://localhost:8080

    # AMQP does not guarantee that a published message is received by
    # the AMQP server, so Galaxy/Pulsar can request that the consumer
    # acknowledge messages and will resend them if acknowledgement is
    # not received after a configurable timeout. 
    #amqp_acknowledge: false

    # Galaxy reuses Pulsar's persistence_directory parameter (via the
    # Pulsar client lib) to store a record of received
    # acknowledgements, and to keep track of messages which have not
    # been acknowledged.
    #persistence_directory: /path/to/dir

    # Number of seconds to wait for an acknowledgement before
    # republishing a message.
    #amqp_republish_time: 30

    # Pulsar job manager to communicate with (see Pulsar
    # docs for information on job managers).
    #manager: _default_

    # The AMQP client can provide an SSL client certificate (e.g. for
    # validation), the following options configure that certificate
    # (see for reference:
    # https://kombu.readthedocs.io/en/latest/reference/kombu.connection.html
    # ). If you simply want to use SSL but not use/validate a client
    # cert, just use the ?ssl=1 query on the amqp URL instead.
    #amqp_connect_ssl_ca_certs: /path/to/cacert.pem
    #amqp_connect_ssl_keyfile: /path/to/key.pem
    #amqp_connect_ssl_certfile: /path/to/cert.pem
    #amqp_connect_ssl_cert_reqs: cert_required
    # By default, the AMQP consumer uses a nonblocking connection with
    # a 0.2 second timeout. In testing, this works fine for
    # unencrypted AMQP connections, but with SSL it will cause the
    # client to reconnect to the server after each timeout. Set to a
    # higher value (in seconds) (or `None` to use blocking connections).
    #amqp_consumer_timeout: None

  pulsar_k8s:
    # Use a two-container Kubernetes to run jobs - one for staging and one
    # for tool execution.
    load: galaxy.jobs.runners.pulsar:PulsarKubernetesCoexecutionJobRunner
    galaxy_url: http://docker.for.mac.localhost:8080
    amqp_url: amqp://guest:guest@localhost:5672//

  pulsar_legacy:
    # Pulsar job runner with default parameters matching those
    # of old LWR job runner. If your Pulsar server is running on a
    # Windows machine for instance this runner should still be used.

    # These destinations still needs to target a Pulsar server,
    # older LWR plugins and destinations still work in Galaxy can
    # target LWR servers, but this support should be considered
    # deprecated and will disappear with a future release of Galaxy.
    load: galaxy.jobs.runners.pulsar:PulsarLegacyJobRunner

handling:
  processes:
    handler0:
    handler1:
    sge_handler:
      # Restrict a handler to load specific runners, by default they will load all.
      plugins: ['sge']
    special_handler0:
      tags: [special_handlers]
    special_handler1:
      tags: [special_handlers]


execution:
  default: local
  environments:
    local:
      runner: local

    multicore_local:
      runner: local
      # Warning: Local slot count doesn't tie up additional worker threads, to prevent over
      # allocating machine define a second local runner with different name and fewer workers
      # to run this destination.
      local_slots: 4
      # Embed metadata collection in local job script (defaults to true for most runners).
      embed_metadata_in_job: true

    docker_local:
      runner: local
      docker_enabled: true

    singularity_local:
      runner: local
      singularity_enabled: true

    # The above Docker and Singularity examples describe how to specify
    # default and override containers but fuller descriptions can be used
    # also to tweak extra options. Like in the above examples, "container_override"
    # will override the tool centric container resolution specified by the container
    # resolvers configuration and "containers" will provide a default if no such
    # container is found during resolution.

    # resolve_dependencies defaults to false, but can be set to true to use
    # dependency resolution inside the container (you'll likely want to ensure
    # Galaxy's tool dependency directory and/or Conda prefix is mounted in the
    # container if this is set. shell (defaults to /bin/sh) can be used to tell
    # Galaxy to use bash for instance in the target contanier.

    # If using these options, docker_enabled and/or singularity_enabled should
    # also be set to true to enable the desired container technology. If multiple
    # such containers are defined (as in the example below), the first one matching
    # the enabled container types for this destination will be used.
    customized_container:
      runner: local
      container:
        - type: docker
          shell: '/bin/sh'
          resolve_dependencies: false
          identifier: 'busybox:ubuntu-14.04'
        - type: singularity
          shell: '/bin/sh'
          resolve_dependencies: false
          identifier: '/path/to/default/container'
      container_override:
        - type: docker
          shell: '/bin/sh'
          resolve_dependencies: false
          identifier: 'busybox:ubuntu-14.04'
        - type: singularity
          shell: '/bin/sh'
          resolve_dependencies: false
          identifier: '/path/to/default/container'
    pbs:
      runner: pbs
      tags: [mycluster]
    pbs_longjobs:
      runner: pbs
      tags: [mycluster, longjobs]
      Resource_List: 'walltime=72:00:00'
    remote_cluster:
      runner: drmaa
      tags: [longjobs]
      # Set to False if cluster nodes don't shared Galaxy library,
      # it will perform metadata calculation locally after the job finishes.
      embed_metadata_in_job: true
      # If jobs are configured to run as the real user, this option allows
      # users that are not mapped to any real users to run jobs
      # as a Galaxy (fallback). Default is false.
      allow_guests: true
    java_cluster:
      runner: drmaa
      env:
        # set arbitrary environment variables at runtime. General
        # dependencies for tools should be configured via
        # tool_dependency_dir and package options and these
        # options should be reserved for defining cluster
        # specific options.
        - name: '_JAVA_OPTIONS'
          value: '-Xmx6G'
        - name: ANOTHER_OPTION
          raw: true  # disable auto-quoting.
          value: "'5'"
        - file: /mnt/java_cluster/environment_setup.sh  # script will be sourced
        - execute: 'module load javastuff/2.10'
        # files to source and exec statements will be handled on remote
        # clusters. These don't need to be available on the Galaxy server
        # itself.

    # Following three environments demonstrate setting up per-job temp directory handling.
    # In these cases TEMP, TMP, and TMPDIR will be set for each job dispatched to these
    # environments.

    # The first simply tells Galaxy to create a temp directory in the job directory, the
    # other forms can be used to issue shell commands before the job runs on the worker node to
    # allocate a temp directory. In these other cases, Galaxy will not clean up these
    # directories so either use directories managed by the job resource manager or setup
    # tooling to clean old temp directories up outside of Galaxy.
    clean_tmp_by_job:
      runner: drmma
      tmp_dir: true
    clean_tmp_drm:
      runner: drmma
      tmp_dir: $DRM_SET_VARIABLES_FOR_THIS_JOB
    clean_tmp_fast_scratch:
      runner: drmma
      tmp_dir: '$(mktemp -d /mnt/scratch/fastest/gxyjobXXXXXXXXXXX)'

    real_user_cluster:
      # The drmaa runner can be used to run jobs as the submitting user,
      # make sure to setup 3 real user parameters in galaxy.yml.
      runner: drmaa

    dynamic:
      runner: dynamic
      # A destination that represents a method in the dynamic runner.
      # foo should be a Python function defined in any file in
      # lib/galaxy/jobs/rules.
      function: foo
    dtd_destination:
      runner: dynamic
      # DTD is a special dynamic job destination type that builds up
      # rules given a YAML-based DSL (see config/tool_destinations.yml.sample
      # for the syntax).
      type: dtd
    load_balance:
      runner: dynamic
       # Randomly assign jobs to various static destination ids
      type: choose_one
      destination_ids: cluster1,cluster2,cluster3
    load_balance_with_data_locality:
      runner: dynamic
      # Randomly assign jobs to various static destination ids,
      # but keep jobs in the same workflow invocation together and
      # for those jobs ran outside of workflows keep jobs in same
      # history together.

      type: choose_one
      destination_ids: cluster1,cluster2,cluster3
      hash_by: workflow_invocation,history
    burst_out:
      runner: dynamic
      # Burst out from static destination local_cluster_8_core to
      # static destination shared_cluster_8_core when there are about
      # 50 Galaxy jobs assigned to any of the local_cluster_XXX
      # destinations (either running or queued). If there are fewer
      # than 50 jobs, just use local_cluster_8_core destination.

      # Uncomment job_state parameter to make this bursting happen when
      # roughly 50 jobs are queued instead.
      type: burst
      from_destination_ids: [local_cluster_8_core, local_cluster_1_core, local_cluster_16_core]
      to_destination_id: shared_cluster_8_core
      num_jobs: 50
      # job_states: queued
    burst_if_queued:
      runner: dynamic
      # Dynamic destinations can be chained together to create more
      # complex rules. In this example, the built-in burst stock rule
      # determines whether to burst, and if so, directs to burst_if_size,
      # a user-defined dynamic destination. This destination in turn will
      # conditionally route it to a remote pulsar node if the input size
      # is below a certain threshold, or route to local if not.
      type: burst
      from_destination_ids: local
      to_destination_id: burst_if_size
      num_jobs: 2
      job_states: queued
    burst_if_size:
      runner: dynamic
      type: python
      function: to_destination_if_size
      # Also demonstrates a destination level override of the
      # rules_module. This rules_module will take precedence over the
      # plugin level rules module when resolving the dynamic function
      rules_module: galaxycloudrunner.rules
      max_size: 1g
      to_destination_id: galaxycloudrunner
      fallback_destination_id: local
    galaxycloudrunner:
      runner: dynamic
      # Demonstrates how to use the galaxycloudrunner, which enables dynamic bursting
      # to cloud destinations. For detailed information on how to use the galaxycloudrunner,
      # consult the documentation at: https://galaxycloudrunner.readthedocs.io/
      type: python
      function: cloudlaunch_pulsar_burst
      rules_module: galaxycloudrunner.rules
      cloudlaunch_api_endpoint: https://launch.usegalaxy.org/cloudlaunch/api/v1
      # Obtain your CloudLaunch token by visiting: https://launch.usegalaxy.org/profile
      cloudlaunch_api_token: 37c46c89bcbea797bc7cd76fee10932d2c6a2389
      # id of the PulsarRESTJobRunner plugin. Defaults to "pulsar"
      pulsar_runner_id: pulsar
      # Destination to fallback to if no nodes are available
      fallback_destination: local
      # Pick next available server and resubmit if an unknown error occurs
      # resubmit: {condition: 'unknown_error and attempt <= 3', environment: galaxycloudrunner}

    docker_dispatch:
      runner: dynamic
      # Follow dynamic destination type will send all tool's that
      # support docker to static destination defined by
      # docker_destination_id (docker_cluster in this example) and all
      # other tools to default_destination_id (normal_cluster in this
      # example).
      type: docker_dispatch
      docker_destination_id: docker_cluster
      default_destination_id: normal_cluster

    # Pulsar enviornment examples
    secure_pulsar_rest_dest:
       runner: pulsar_rest
       # URL of Pulsar server.
       url: https://examle.com:8913/
       
       # If set, private_token must match token in remote Pulsar's
       # configuration.
       private_token: 123456789changeme
       
       # Uncomment the following statement to disable file staging (e.g.
       # if there is a shared file system between Galaxy and the Pulsar
       # server). Alternatively action can be set to 'copy' - to replace
       # http transfers with file system copies, 'remote_transfer' to cause
       # the Pulsar to initiate HTTP transfers instead of Galaxy, or
       # 'remote_copy' to cause Pulsar to initiate file system copies.
       # If setting this to 'remote_transfer' be sure to specify a
       # 'galaxy_url' attribute on the runner plugin above. -->
       default_file_action: none

       # The above option is just the default, the transfer behavior
       # none|copy|http|remote_transfer|remote_copy can be configured on a per
       # path basis via the following file or dictionary. See Pulsar documentation
       # for more details and examples.
       #file_action_config: file_actions.yaml
       #file_actions: {}
       
       # The non-legacy Pulsar runners will attempt to resolve Galaxy
       # dependencies remotely - to enable this set a tool_dependency_dir
       # in Pulsar's configuration (can work with all the same dependency
       # resolutions mechanisms as Galaxy - tool Shed installs, Galaxy
       # packages, etc...). To disable this behavior, set the follow parameter
       # to none. To generate the dependency resolution command locally
       # set the following parameter local.
       #dependency_resolution: none

       # Uncomment following option to enable setting metadata on remote
       # Pulsar server. The 'use_remote_datatypes' option is available for
       # determining whether to use remotely configured datatypes or local
       # ones (both alternatives are a little brittle). 
       #remote_metadata: true
       #use_remote_datatypes: false
       #remote_property_galaxy_home: /path/to/remote/galaxy-central
       
       # If remote Pulsar server is configured to run jobs as the real user,
       # uncomment the following line to pass the current Galaxy user
       # along. 
       #submit_user: $__user_name__
       
       # Various other submission parameters can be passed along to the Pulsar
       # whose use will depend on the remote Pulsar's configured job manager.
       # For instance:
       #submit_native_specification: -P bignodes -R y -pe threads 8
       
       # Disable parameter rewriting and rewrite generated commands
       # instead. This may be required if remote host is Windows machine
       # but probably not otherwise.
       #rewrite_parameters: false

    pulsar_mq_dest:
       runner: pulsar_mq
       # The RESTful Pulsar client sends a request to Pulsar
       # to populate various system properties. This
       # extra step can be disabled and these calculated here
       # on client by uncommenting jobs_directory and
       # specifying any additional remote_property_ of
       # interest, this is not optional when using message
       # queues.
       jobs_directory: /path/to/remote/pulsar/files/staging/
       # Otherwise MQ and Legacy pulsar destinations can be supplied
       # all the same destination parameters as the RESTful client documented
       # above (though url and private_token are ignored when using a MQ).

    pulsar_k8s_environment:
      runner: pulsar_k8s
      docker_enabled: true  # probably shouldn't be needed but is still
      docker_default_container_id: 'conda/miniconda2'
      # Specify a non-default Pulsar staging container.
      #pulsar_container_image: 'galaxy/pulsar-pod-staging:0.12.0'

    # Example CLI runners.
    ssh_torque:
      runner: cli
      shell_plugin: SecureShell
      job_plugin: Torque
      shell_username: foo
      shell_hostname: foo.example.org
      job_Resource_List: walltime=24:00:00,ncpus=4

    ssh_slurm:
      runner: cli
      shell_plugin: SecureShell
      job_plugin: Slurm
      shell_username: foo
      shell_hostname: my_host
      job_time: 2:00:00
      job_ncpus: 4
      job_partition: my_partition

    local_lsf_8cpu_16GbRam:
      runner: cli
      shell_plugin: LocalShell
      job_plugin: LSF
      job_memory: 16000
      job_cores: 8
      job_project: BigMem

# Tools can be configured to use specific destinations or handlers,
# identified by either the "id" or "tags" attribute.  If assigned to
# a tag, a handler or destination that matches that tag will be
# chosen at random. 
tools:
- id: bwa
  handler: handler0
- id: bowtie
  handler: handler1
- id: bar
  enviroment: dynamic
- 
  # Next example defines resource group to insert into tool interface
  # and pass to dynamic destination (as resource_params argument).
  id: longbar
  environment: dynamic
  resources: all
- 
  # Pick a handler randomly from those declaring this tag.
  id: baz
  handler: special_handlers
  environment: bigmem
- 
  # legacy trackerster parameter for tool mapping
  id: foo
  handler: handler0
  source: trackster

resources:
  default: default
  groups:
    # Group different parameters defined in job_resource_params_conf.xml
    # together and assign these groups ids. Tool section below can map
    # tools to different groups.
    default: []
    memoryonly: [memory]
    all: [processors, memory, time, project]

# Certain limits can be defined. The 'concurrent_jobs' limits all
# control the number of jobs that can be "active" at a time, that
# is, dispatched to a runner and in the 'queued' or 'running'
# states.

# A race condition exists that will allow environment_* concurrency
# limits to be surpassed when multiple handlers are allowed to
# handle jobs for the same environment. To prevent this, assign all
# jobs for a specific environment to a single handler.
limits:
- 
  # Limit on the number of jobs a user with a registered Galaxy
  # account can have active across all environments.
  type: registered_user_concurrent_jobs
  value: 2

- 
  # Likewise, but for unregistered/anonymous users.
  type: anonymous_user_concurrent_jobs
  value: 1

- 
  # The number of jobs a user can have active in the specified
  # environment, or across all environments identified by the
  # specified tag. (formerly: concurrent_jobs)
  type: environment_user_concurrent_jobs
  id: local
  value: 1

-
  type: environment_user_concurrent_jobs
  tag: mycluster
  value: 2

-
  type: environment_user_concurrent_jobs
  tag: longjobs
  value: 1

-
  # The number of jobs that can be active in the specified
  # environment (or across all environments identified by the
  # specified tag) by any/all users.
  type: environment_total_concurrent_jobs
  id: local
  value: 16

-
  type: environment_total_concurrent_jobs
  tag: longjobs
  value: 100

-
  # Amount of time a job can run (in any environment) before it
  # will be terminated by Galaxy.
  type: walltime
  value: '24:00:00'

- 
  # Total walltime that jobs may not exceed during a set period.
  # If total walltime of finished jobs exceeds this value, any
  # new jobs are paused.  `window` is a number in days,
  # representing the period.
  type: total_walltime
  window: 30
  value: '24:00:00'

- 
  # Size that any defined tool output can grow to before the job
  # will be terminated. This does not include temporary files
  # created by the job. Format is flexible, e.g.:
  # '10GB' = '10g' = '10240 Mb' = '10737418240'
  type: output_size
  value: '10GB'
